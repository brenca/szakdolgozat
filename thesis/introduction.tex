%----------------------------------------------------------------------------
\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}
%----------------------------------------------------------------------------

The main goal of this thesis is to create a modular interpreter in the native language of the modern Internet, JavaScript. The idea behind such an interpreter is that it is a hassle to download and install the various compilers and IDEs needed to even begin the actual programming phase, and this could be made easier with web technologies.

Nowadays, everyone has a laptop or a smartphone (most of the times both) capable of browsing the World Wide Web, and all these devices have a browser to achieve this task. Most dynamic websites use JavaScript as a language to manipulate the content of the page (to make it dynamic), so all these browsers have a JavaScript engine built into them. I intend to use this capability to bring a more seamless first experience to those who want to learn programming and are at the beginning of their journey. 

This is why I set out to create an interpreter that is:
\begin{itemize}
\item Written in JavaScript, thus available to be used on every kind of device
\item Modular, thus making the job of creators of educational tools easier by abstracting the common things, and providing an easy way to allow execution of code inside the browser in any custom language that has a module created for it
\end{itemize}

To execute code written in a specific language, either a compiler (that translates directly into machine code, which is then executed), or an interpreter is used (which is a program that actively interprets the given source code line-by-line). JavaScript is an interpreted language --- that is one of the reasons it can be found in so many places --- and thus the interpreter described in this thesis is a cross-interpreter --- that is to say it translates from an interpreted language to another interpreted language. But before any interpretation can begin, the code has to go through some transformations, mainly lexical analysis and syntactical analysis.

Lexical analysis is basically turning the source code into tokens using basic pattern matching rules, which makes the parsing stage easier. A token belongs to a class of tokens, which represent the meaning of the value of the token. The value of the token is a specific part of the input code that is paired with the class by pattern matching. A good way to do pattern matching is using regular expressions, which is a pattern notation technique that has a parser available in JavaScript as part of the language --- which provides a convenient way to implement this feature of the analyser. In case the writer of a module needs more complex rules, lexical analysers usually provide a way to use simple states and define transitions between these states to express these rules easier.

In the next stage called parsing (or syntactical analysis), the tokens are turned into a parse tree, using a formal grammar. A parse tree represents the hierarchy of matched rules. A common way to define the grammar is BNF (Backus-Naur form), which is a simple notation to describe grammars and their rules. The given grammar describes every type of statement in the language. In the definitions of the rules, the writer of the grammar can reference the token classes from the previous stage (rules are automatically added to the grammar to represent the classes). The parse tree then can be used to create an AST (Abstract Syntax Tree), which is a hierarchical representation of the input's structure. The AST then can be used to interpret the input itself by calculating the values of the tree's nodes, which basically represent a statement and the parameters of the statement. Some of these parameters have fixed values, while others need some computation to produce a value first (which means repeating the same algorithm that calculates the value for a node for the parameter's node itself).

To create a language module, the necessary steps are:
\begin{enumerate}
\item Creating the rules for the lexical analyser (defining token classes and the state machine if necessary)
\item Providing the grammar in BNF
\item Parsing the code, and iterating over the nodes of the produced AST
\end{enumerate}
The last step is the actual interpretation, which can be a simple evaluation of an expression, or even a small virtual machine that keeps track of functions, variables, scopes and etc. This is the part that requires most attention when writing a language module.

Of course, something like this has been attempted many times. There are numerous solutions to creating parsers, although most of them are not written using JavaScript, they are native tools (written in compiled languages) to generate parsers. The list of some of the more popular tools include GNU Bison \footnote{GNU Bison \url{https://www.gnu.org/software/bison/}}, Jison \footnote{Jison - Your friendly JavaScript parser generator! \url{http://zaa.ch/jison/}} (a clone of Bison in JavaScript), YACC \footnote{Yet Another Compiler-Compiler \url{http://dinosaur.compilertools.net/}} and ANTLR \footnote{Another Tool For Language Recognition \url{http://www.antlr.org/}}. But out of all of the available works, the most relevant to this thesis is Syntax \footnote{Syntax - language agnostic parser generator \url{https://medium.com/@DmitrySoshnikov/syntax-language-agnostic-parser-generator-bd24468d7cfc}}, which has the same initial goals (to create a parser generator in JavaScript) and the same things in mind (educational purposes) as the idea behind this thesis. A blog post about Syntax was published just a week after the proposal for this thesis was handed in. The author of Syntax describes his motives in the post and the main differences from my thesis are the following:
\begin{itemize}
\item He only set out to create a modular (language agnostic) parser generator
\item He provides multiple parsing algorithms
\item He provides multiple target languages, including JavaScript, but that only means that the parser that is generated is implemented in JavaScript (other targets include Python, Ruby and PHP)
\item He provides a way to set operator precedence
\item He does not provide a state based lexical analyser
\end{itemize}
In every other sense, the two projects are very similar, although I have only learned about Syntax during later stages of the development of my solution.

Another similar project is an older project of mine --- from which the idea came for this thesis ---, which is a Logo interpreter that translates the code to JavaScript, and which was based on absolutely no research about parsers, and thus is very complicated and sometimes buggy --- but generally works quite well. This project had two versions, the first of which I built 4 years ago as a hobby --- to help a friend of mine with her Logo homework (since the interpreter used in her class was a 16 bit program, and she had a 64 bit computer which made it impossible to run the interpreter), and the second version that was built on the experience that came from building the first --- so it was much more mature, and could handle Logo source reasonably well. The choice of language --- other than the friend I mentioned --- is because it is a good step towards understanding basic ideas of programming (in both functional and imperative styles), and also provides the user with something materialistic as a result (it is easy to create beautiful and complex drawings), which is great for young children to get a reward for their work, and a great way to motivate them. One of the most beautiful things someone can draw with Logo easily is fractals (up to a certain level of depth), which also teach them a lot about recursion, which can be hard to understand for new pupils (fractals are a good way to visualize recursion).

Other solutions to running code on the web include cloud based solutions, which means that the code that the user inputs is actually compiled and ran on a remote virtual machine. While this is great in the sense that it is exactly as the "real thing", it has a great drawback - it requires an active connection to the Internet to use. My solution can be bundled into a desktop application (or the website can be downloaded) using tools like the electron \footnote{Electron \url{http://electron.atom.io/}} framework, or it can be integrated into a website. This allows the students to practice while on the go (and without an active Internet connection), which cloud based solutions can not allow.

During the implementation, speed was not the main focus, since this is not a tool built for performance but rather one that is built to be available everywhere --- but I did try to not be wasteful with the resources. To achieve modern, well readable source code and portability, I used the ECMAScript 6 standard, which is essentially a JavaScript standard that most modern browsers can interpret -- but only the newer versions. I developed my solution in Node.js modules, which can then be converted into code that works on older browsers using Babel \footnote{Babel \url{https://github.com/babel/babel}} and Browserify \footnote{Browserify \url{http://browserify.org/}}, which are Node.js modules themselves to convert to older standards and to bundle modules into one source file for browsers respectively.

Chapter 1 will outline the basic coding conventions I followed during development. In Chapter 2, I will describe the implemented lexical analyser, and it's inner workings. In Chapter 3, I will write about the parser module, and the implemented parsing algorithm. In Chapter 4, I will describe the two example modules I have created - a math expression parser, and a simple Logo subset interpreter. In Chapter 5 I will give a summary of the chapters before and some conclusions while in Chapter 6 I will write about problems and possible improvements for the future.
