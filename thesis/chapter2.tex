%----------------------------------------------------------------------------
\chapter{Lexical analysis}
%----------------------------------------------------------------------------
Lexical analysis is a process in which an input string is turned into tokens. This is done to make the next step --- parsing --- easier. For example, the parser can deal with words instead of trying to match the grammar's rules character-by-character. This can greatly reduce the complexity of the grammar, which means it makes it easier to write a grammar for a specific language.
%----------------------------------------------------------------------------
\section{Using regular expressions}
%----------------------------------------------------------------------------
Using regular expressions is a common technique used in lexical analysis (e.g. the lexical analyser of the popular parser generator Yacc uses this technique \cite{Lex}), since during the analysis, we need to determine what class of tokens does the beginning of the given source string translates to --- and regular expressions provide a simple way to write patterns that we can match against our input.

The use of regular expressions is a well established way of pattern matching. The concept was created in the 1950s by Stephen Cole Kleene \cite{kleene1951representation}. A regular expression is a string of characters that describe a pattern, which then can be matched against a string to do various operations like finding the first matching position, finding all matching positions, replacing said matches with other strings, etc. The language of regular expressions has numerous rules which allows us to create very complicated patterns. The regular expression engine that JavaScript implements is well documented \footnote{JavaScript RegExp reference \url{https://developer.mozilla.org/en/docs/Web/JavaScript/Guide/Regular_Expressions}}, with most of the features of regular expressions available to the programmer. As an example, here are some patterns and their meaning:
\begin{itemize}
\item \texttt{/[0-9]+\textbackslash.[0-9]+/} match strings looking like floating point numbers (a dot indicates the decimal point)
\item \texttt{/[a-z]+[a-z0-9]*/i} match a string that must start with a character, has to be at least 1 character long and otherwise can contain alphanumeric characters (lower and upper case too)
\item \texttt{/[0-9]+(?![0-9]*\textbackslash.[0-9]+)/} match strings that represent an integer (a sequence of numbers that is not followed by any more numbers and a dot)
\end{itemize}

The first pattern makes use of ranges. Instead of having to write out all the numbers, we can just say \texttt{[0-9]}, which will match any number. Anything in a pair of square brackets is a group of possible characters in that position, for example \texttt{[abc]} would only match \texttt{a}, \texttt{b} or \texttt{c}. The \texttt{+} sign is a quantifier, expressing that at least one character from the group before it must be present. We can see another quantifier in the second rule, the \texttt{*} (which is called a Kleene star) means zero or more matches of the group it is attached to. Of course these quantifiers can be attached to single symbols too, in which case the square brackets are not needed. In the first and the last group, we can observe a \texttt{\textbackslash.} combination. This is required because \texttt{.} has a special meaning in regular expressions (it matches any character), so if we want to only match a dot, we need to escape it, which is done by adding a \texttt{\textbackslash} before it. In the second rule, we can see a modifier at the end of the expression. The modifier \texttt{i} means that the matching should be done in a case-insensitive way (the default mode is case-sensitive). In the last rule, we can see one of the more complicated kinds of expressions - at the end of the expression, there is a pattern \texttt{(?![0-9]*\textbackslash.[0-9]+)}, which is a negative lookahead pattern. This means that the whole expression will only match if the inside of this pattern (some numbers, a dot and some more numbers in this case) can not be matched after the examined string. The negative lookahead will not be included in the result. Several other tools are available in regular expressions to write complicated patterns (such as positive lookahead, capture groups, matching exactly n times, etc.), which can make lexical analysis easier, since the regular expression engine is doing some of the work for us. The JavaScript regular expression engine does not however implement lookbehinds, which is the same concept as lookaheads, but instead peeking backwards in the input.

Because of the expressiveness of regular expressions, my implementation of the lexical analyser will be fairly simple, without the need of explicitly working with lookahead symbols when transitioning between states and matching strings to their classes, since these special rules can be handled by carefully writing the regular expressions.
%----------------------------------------------------------------------------
\section{The Lexer module}
%----------------------------------------------------------------------------
The lexical analyser module is named Lexer --- which is a JavaScript class with several subclasses. Lexer implements a regular expression based matching system with an additional ability to specify states, and transitions between these states. This is optional however, and Lexer can work without keeping track of states.

\subsection{Lexer subclasses}
\subsubsection{Lexer.Token}
The Token class represents a token --- it has a value, which is the parsed string that belongs to the token, and a class, which is a reference to the TokenClass the token belongs to.
\subsubsection{Lexer.TokenClass}
The TokenClass represents a class of tokens --- it has a name, an associated regular expression, and a state, which defaults to the DefaultState. It also has a match method that tries to match against a given string, and returns the consumed part of the string alongside with the remaining part. If the regular expression can not be successfully matched against the string, the consumed part will have a value of false, indicating no successful match.
Two special subclasses are always added to the list of classes, which are the EOLTokenClass --- it matches the end-of-line characters --- and the EOFTokenClass --- which matches the end-of-file indicator.
Another special subclass is the StatelessTokenClass, which associates the AnyState with itself, which means it will match in any non-strict state.
\subsubsection{Lexer.TokenClassGroup}
The TokenClassGroup is a container for TokenClasses. It can be used to define transitions if any of the classes in it is matched. It has an interface that consists of a push and pop method, a mapping function to transform all elements and a find function that checks if a TokenClass is part of the group or not.
\subsubsection{Lexer.State}
The State class represents a state in stateful mode --- it has a name, and a strict parameter, which is false by default.
Two special subclasses exist, one is the AnyState, which is used for tokens that should match in any state that is non-strict, and the other is the DefaultState, which is the one the state machine starts in, and which all classes belong to if no state parameter is given to them.
\subsubsection{Lexer.StateTransitionBase}
This class is the common core of the state transition classes --- it has a from and to member, which represent the states to transition from and to, and it also has a method that takes the Lexer, and checks and retrieves the states for the two members (initially, the to and from members contain the name of the states only).
\subsubsection{Lexer.StateTransition}
The StateTransition class is the traditional state transition, which has a token class member alongside the inherited to and from. The instances can be created with two or three parameters --- in the case of two parameters, the from state will be derived from the state associated with the token class, while if the class and the two states are provided, the from state will be explicitly set. This allows state transitions based on stateless token classes.
\subsubsection{Lexer.GroupStateTransition} 
The GroupStateTransition is similar to the StateTransition class, but it defines a transition that happens when any of the group's token classes match in a certain state. The three parameters are the TokenClassGroup, the from and the to states.
\subsection{Stateless mode}
In stateless mode, Lexer only uses the default state, and does not transition into any other state --- and therefore the token classes do not need to be initialised with data about which state they belong to.
\subsection{Stateful mode}
In stateful mode, Lexer takes advantage of the rules defined by the States, StateTransitions and GroupStateTransitions given by the user. Lexer keeps track of the current state, and prioritizes group based transitions when deciding what state to transition to.
\subsection{Skipping whitespaces}
To make the lexical analysis easier, Lexer can be told to ignore whitespace characters. This is a very useful option, and so the default value is true --- but some languages require us to keep track of the indentation (e.g. Python), in which case the rules have to deal with the whitespace characters.
\subsection{The Lexer class}
The Lexer class is the core of this module. It provides facilities to add states, transitions, token classes, and has a method that runs the analysis on the given string. This method is called tokenize, and it follows an iterative algorithm --- it looks for the first rule that matches the beginning of the string, creates a token based on that rule, does the necessary state transitions and starts the process all over again. The return value is a dictionary with three values:
\begin{itemize}
\item success - indicates whether the whole string was consumed or not
\item tokens - the list of tokens the process generated
\item rest - the rest of the string that was not parsed, if any
\end{itemize}
%----------------------------------------------------------------------------
\section{Possible enhancements}
%----------------------------------------------------------------------------
In the current state, the Lexer module is easily usable --- however, the user needs to define first the states, then the token classes and groups and finally the transitions. This is a quite verbose way of defining the rules for the analysis --- which could be done by using a custom data format that describes the rules. This data could be read from a file and preprocessed, which could make the process of defining the rules more robust. It could also be a more compact way of describing the rules.

Currently, the Lexer module can not transform the value of the tokens it creates. This could also make parsing easier, since e.g. integers and floats could be transformed into their numerical values during this stage, rather than having to deal with this process during the traversal of the AST.